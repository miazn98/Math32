{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import imshow, show, get_cmap\n",
    "from numpy import random\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import operator\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "from skimage.measure import label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generating Dataset and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random dataset of 12x12 binary array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets generate 5,000 random 4 by 4s\n",
    "\n",
    "# rand_array is the key to convert to a black and white image object\n",
    "# Binary array is the key to figure out euler number\n",
    "\n",
    "\n",
    "len_data = 1\n",
    "# This creates test_4, which is the nonbinary random array\n",
    "i = 0\n",
    "#return tensor of samples\n",
    "test_list = []\n",
    "while i <= (len_data):\n",
    "    Z = random.random((12,12))\n",
    "    Y = xr.DataArray(Z)\n",
    "\n",
    "    test_list.append(Y)\n",
    "    i += 1\n",
    "test_4 = xr.concat(test_list, dim = \"DataArray\")\n",
    "len(test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates a dataarray object of binary arrays\n",
    "\n",
    "bin_list = []\n",
    "for i in range (0, (len_data)):\n",
    "    bin_pre_arr = np.array(test_4[i])\n",
    "    img = Image.fromarray(bin_pre_arr, '1')\n",
    "    #img.show()\n",
    "    bin_arr = np.array(img)\n",
    "    bin_arr = bin_arr.astype(np.int)\n",
    "    bin_arr_xr = xr.DataArray(bin_arr)\n",
    "    bin_list.append(bin_arr_xr)\n",
    "    \n",
    "test_4_bin = xr.concat(bin_list, dim = \"DataArray\")\n",
    "\n",
    "# now we need to add the enclosure\n",
    "test_4_bin\n",
    "\n",
    "    \n",
    "    \n",
    "test_4_bin_list = []\n",
    "for i in range(0, (len_data)):\n",
    "    final_bin = np.array(test_4_bin[i])\n",
    "\n",
    "    for i in range(0, (np.size(final_bin, 1))):\n",
    "        final_bin[0, i] = 0\n",
    "        final_bin[- 1, i] = 0\n",
    "        final_bin[i, 0] = 0\n",
    "        final_bin[i, -1] = 0\n",
    "        final_bin_xr = final_bin.astype(np.int)\n",
    "        final_bin_xr = xr.DataArray(final_bin_xr)\n",
    "    test_4_bin_list.append(final_bin_xr)\n",
    "\n",
    "test_4_bin_final = xr.concat(test_4_bin_list, dim = \"DataArray\")\n",
    "\n",
    "\n",
    "len(test_4_bin_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of connected 0's: [5]\n",
      "count of connected 1's: [3]\n",
      "Euler Characteristic: [2]\n"
     ]
    }
   ],
   "source": [
    "zero_counter_list = []\n",
    "one_counter_list = []\n",
    "for i in range(0, (len_data)):\n",
    "\n",
    "    img_arr = np.array(test_4_bin_final[i])\n",
    "    \n",
    "    # Set \"connectivity\" to 1 for 4-connection, 2 for 8-connection (one's and zero's should have diff values for \n",
    "    # connectivity)\n",
    "    one_labels = label(img_arr, connectivity=1 , background=-1)  \n",
    "\n",
    "    one_label_vals = np.unique(one_labels)                      \n",
    "    one_counter = 0\n",
    "    for i in one_label_vals:\n",
    "        one_indices = np.where(one_labels == i)\n",
    "        if one_indices:\n",
    "            if img_arr[one_indices][0] == 0:\n",
    "            \n",
    "                one_counter += 1\n",
    "    one_counter_list.append(one_counter)\n",
    "    \n",
    "    img_arr_inv = img_arr\n",
    "    for i in range(0, (np.size(img_arr, 1))):\n",
    "        for j in range(0, (np.size(img_arr, 1))):\n",
    "            if img_arr_inv[i][j] == 0:\n",
    "                img_arr_inv[i][j] = 1\n",
    "            elif img_arr_inv[i][j] == 1:\n",
    "                img_arr_inv[i][j] = 0\n",
    "                \n",
    "                \n",
    "    # now run the same counter on the inverse matrix (to get number of connected black regions)\n",
    "    \n",
    "    # Set \"connectivity\" to 1 for 4-connection, 2 for 8-connection\n",
    "    zero_labels = label(img_arr_inv, connectivity=2, background=-1)  \n",
    "    zero_label_vals = np.unique(zero_labels)                      \n",
    "    zero_counter = 0\n",
    "    for i in zero_label_vals:\n",
    "        zero_indices = np.where(zero_labels == i)\n",
    "        if zero_indices:\n",
    "            if img_arr_inv[zero_indices][0] == 0:\n",
    "            #print('hole: ', indices)\n",
    "                zero_counter += 1\n",
    "    \n",
    "    zero_counter_list.append(zero_counter)\n",
    "    \n",
    "counter_list = list(map(operator.sub, one_counter_list, zero_counter_list))\n",
    "    \n",
    "    \n",
    "print(\"count of connected 0's: \" + str(one_counter_list))\n",
    "print(\"count of connected 1's: \" + str(zero_counter_list))\n",
    "print(\"Euler Characteristic: \" + str(counter_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run below code if saving dataarray to file\n",
    "import netCDF4\n",
    "\n",
    "#new_filename_3 = './data_iter3.nc'\n",
    "#print ('saving to ', new_filename_3)\n",
    "\n",
    "#test_4_bin_final.to_netcdf(path=new_filename_3)\n",
    "#print ('finished saving')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Building the networks in tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pylab import imshow, show, get_cmap\n",
    "from numpy import random\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from PIL import Image\n",
    "import xarray as xr\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import netCDF4\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load large dataset for first iteration of tree\n",
    "data_1 = xr.open_dataarray(\"data_iter3.nc\")\n",
    "data_1 = np.array(data_1)\n",
    "nn_data = pd.DataFrame()\n",
    "for i in range(0, len(data_1)):\n",
    "    row = data_1[i].flatten()\n",
    "    row = pd.Series(row)\n",
    " \n",
    "    nn_data = nn_data.append(row, ignore_index=True)\n",
    "    \n",
    "labels = np.load(\"labels_iter3.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append labels to data\n",
    "labels = pd.Series(labels)\n",
    "nn_data = pd.concat([nn_data, labels], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = nn_data[ (nn_data.iloc[:, -1] > 3) | (nn_data.iloc[:, -1] < -4)  ].index\n",
    "nn_data2 = nn_data\n",
    "nn_data2.drop(names , inplace=True)\n",
    "nn_data2 = nn_data2.dropna()\n",
    "nn_data2.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_data2_bin = []\n",
    "for i in range(0, len(nn_data2)):\n",
    "    if nn_data2.iloc[i, -1] < 0:\n",
    "        nn_data2_bin.append(0)\n",
    "    else:\n",
    "        nn_data2_bin.append(1)\n",
    "nn_data2['binary_label'] = nn_data2_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nn_data2.iloc[:,0:144]\n",
    "Y = nn_data2.iloc[:, -1]\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First Model\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(250, input_dim=144, activation='relu'))\n",
    "model_1.add(Dropout(0.2))\n",
    "model_1.add(Dense(95, activation='relu'))\n",
    "model_1.add(Dropout(0.2))\n",
    "model_1.add(Dense(20, activation='relu'))\n",
    "model_1.add(Dropout(0.2))\n",
    "model_1.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "rms = optimizers.RMSprop(learning_rate=0.0001, rho=0.9)\n",
    "model_1.compile(loss='binary_crossentropy', optimizer= 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, Y, train_size=0.70,\n",
    "                                                                    test_size=0.30)\n",
    "\n",
    "\n",
    "class_weights_1 = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train1),\n",
    "                                                 y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.fit(np.array(X_train1), np.array(y_train1) ,validation_data=(np.array(X_test1),np.array(y_test1)),\n",
    "            class_weight = class_weights_1, epochs=150, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_1.evaluate(X_test1, y_test1, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model_1.metrics_names[1], scores[1]*100))\n",
    "# save model and architecture to single file\n",
    "model_1.save(\"model_1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row2_a = pd.read_csv(\"row2_a.csv\")\n",
    "row2_b = pd.read_csv(\"row2_b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for row2_a (values are between -6 and -1)\n",
    "\n",
    "X_2a = row2_a.iloc[:,1:145]\n",
    "Y_2a = row2_a.iloc[:, -1]\n",
    "X_2a = np.array(X_2a)\n",
    "Y_2a = np.array(Y_2a)\n",
    "\n",
    "# Data for row2_b (values are between 0 and 5)\n",
    "\n",
    "X_2b = row2_b.iloc[:,1:145]\n",
    "Y_2b = row2_b.iloc[:, -1]\n",
    "X_2b = np.array(X_2b)\n",
    "Y_2b = np.array(Y_2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models for second layer of tree\n",
    "\n",
    "model_2a = Sequential()\n",
    "model_2a.add(Dense(200, input_dim=144, activation='relu'))\n",
    "model_2a.add(Dropout(0.3))\n",
    "model_2a.add(Dense(90, activation='relu'))\n",
    "#model_2a.add(Dropout(0.2))\n",
    "model_2a.add(Dense(30, activation='relu'))\n",
    "model_2a.add(Dropout(0.2))\n",
    "model_2a.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "opt = optimizers.SGD(lr=0.0001)\n",
    "rms = optimizers.RMSprop(learning_rate=0.0005, rho=0.9)\n",
    "model_2a.compile(loss='binary_crossentropy', optimizer= rms, metrics=['accuracy'])\n",
    "\n",
    "model_2b = Sequential()\n",
    "model_2b.add(Dense(200, input_dim=144, activation='relu'))\n",
    "model_2b.add(Dropout(0.3))\n",
    "model_2b.add(Dense(90, activation='relu'))\n",
    "model_2b.add(Dropout(0.2))\n",
    "model_2b.add(Dense(40, activation='relu'))\n",
    "model_2b.add(Dropout(0.2))\n",
    "model_2b.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "opt = optimizers.SGD(lr=0.0001)\n",
    "rms = optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "model_2b.compile(loss='binary_crossentropy', optimizer= \"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2a, X_test_2a, y_train_2a, y_test_2a = train_test_split(X_2a, Y_2a, train_size=0.60,\n",
    "                                                                    test_size=0.40)\n",
    "\n",
    "X_train_2b, X_test_2b, y_train_2b, y_test_2b = train_test_split(X_2b, Y_2b, train_size=0.70,\n",
    "                                                                    test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_2a = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train_2a),\n",
    "                                                 y_train_2a)\n",
    "\n",
    "class_weights_2b = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train_2b),\n",
    "                                                 y_train_2b)\n",
    "\n",
    "class_weight_2a = {0:75, 1:24}\n",
    "\n",
    "class_weight_2b = {0:32.4, 1:67.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2a.fit((X_train_2a), (y_train_2a) ,validation_data=((X_test_2a),(y_test_2a)),  class_weight=class_weight_2a,\n",
    "                                                                  epochs=200, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = model_2a.predict_classes(X_test_2a)\n",
    "test_2a = pd.DataFrame(y_test_2a)\n",
    "test_2a['predict'] = vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(0, len(test_2a)):\n",
    "    if test_2a.iloc[i, 0] == test_2a.iloc[i, 1]:\n",
    "        count += 1\n",
    "        \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2b.fit((X_train_2b), (y_train_2b) ,validation_data=((X_test_2b),(y_test_2b)), class_weight=class_weight_2b, \n",
    "                                                                  epochs=150, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_2a.evaluate(X_test_2a, y_test_2a, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model_2a.metrics_names[1], scores[1]*100))\n",
    "# save model and architecture to single file\n",
    "model_2a.save(\"model_2a.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_2b.evaluate(X_test_2b, y_test_2b, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model_2b.metrics_names[1], scores[1]*100))\n",
    "# save model and architecture to single file\n",
    "model_2b.save(\"model_2b.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to one_hot encode these dataframes\n",
    "row3_a = pd.read_csv(\"row3_a.csv\")\n",
    "row3_b = pd.read_csv(\"row3_b.csv\")\n",
    "row3_c = pd.read_csv(\"row3_c.csv\")\n",
    "row3_d = pd.read_csv(\"row3_d.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_3a_final = np.array(row_3a_final)\n",
    "row3_b = np.array(row3_b)\n",
    "row3_c = np.array(row3_c)\n",
    "row_3d_final = np.array(row_3d_final)\n",
    "\n",
    "row_3_test = np.concatenate((row_3a_final, row3_b, row3_c, row_3d_final), axis = 0)\n",
    "row_3_test = pd.DataFrame(row_3_test)\n",
    "row_3_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_3a = row_3_test.iloc[:, -1].between(-4, -3, inclusive = True) \n",
    "index_3b = row_3_test.iloc[:, -1].between(-2, -1, inclusive = True) \n",
    "index_3c = row_3_test.iloc[:, -1].between(0, 1, inclusive = True) \n",
    "index_3d = row_3_test.iloc[:, -1].between(2, 3, inclusive = True) \n",
    "\n",
    "\n",
    "\n",
    "row_3a_s = row_3_test[index_3a]\n",
    "row_3b_s = row_3_test[index_3b]\n",
    "row_3c_s = row_3_test[index_3c]\n",
    "row_3d_s = row_3_test[index_3d]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform x and y for smaller subset\n",
    "\n",
    "# Row 3a (add 4 bc negative outcomes)\n",
    "X_3a = row_3a_s.iloc[:,0:144]\n",
    "Y_3a = row_3a_s.iloc[:, -1]\n",
    "X_3a = np.array(X_3a)\n",
    "Y_3a = np.array(Y_3a) + 4\n",
    "Y_3a = Y_3a.astype(int)\n",
    "\n",
    "# Row 3b (add 2 bc negative outcomes)\n",
    "\n",
    "X_3b = row_3b_s.iloc[:,0:144]\n",
    "Y_3b = row_3b_s.iloc[:, -1]\n",
    "X_3b = np.array(X_3b)\n",
    "Y_3b = np.array(Y_3b) + 2\n",
    "Y_3b = Y_3b.astype(int)\n",
    "\n",
    "# Row 3c \n",
    "X_3c = row_3c_s.iloc[:,0:144]\n",
    "Y_3c = row_3c_s.iloc[:, -1]\n",
    "X_3c = np.array(X_3c)\n",
    "Y_3c = np.array(Y_3c) \n",
    "Y_3c = Y_3c.astype(int)\n",
    "\n",
    "# Row 3d (to start from 0)\n",
    "X_3d = row_3d_s.iloc[:,0:144]\n",
    "Y_3d = row_3d_s.iloc[:, -1]\n",
    "X_3d = np.array(X_3d)\n",
    "Y_3d = np.array(Y_3d) \n",
    "Y_3d = Y_3d.astype(int) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models in row 3\n",
    "\n",
    "model_3a = Sequential()\n",
    "model_3a.add(Dense(250, input_dim=144, activation='relu'))\n",
    "#model_3a.add(Dropout(0.2))\n",
    "#model_3a.add(Dense(95, activation='relu'))\n",
    "#model_3a.add(Dropout(0.2))\n",
    "#model_3a.add(Dense(60, activation='relu'))\n",
    "#model_3a.add(Dropout(0.2))\n",
    "#model_3a.add(Dense(20, activation='relu'))\n",
    "model_3a.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "rms = optimizers.RMSprop(learning_rate=0.0005, rho=0.9)\n",
    "model_3a.compile(loss='binary_crossentropy', optimizer=rms, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Row 2\n",
    "model_3b = Sequential()\n",
    "model_3b.add(Dense(250, input_dim=144, activation='relu'))\n",
    "#model_3b.add(Dropout(0.2))\n",
    "#model_3b.add(Dense(95, activation='relu'))\n",
    "#model_3b.add(Dropout(0.2))\n",
    "#model_3b.add(Dense(60, activation='relu'))\n",
    "model_3b.add(Dropout(0.2))\n",
    "model_3b.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "model_3b.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Row 3\n",
    "model_3c = Sequential()\n",
    "model_3c.add(Dense(250, input_dim=144, activation='relu'))\n",
    "model_3c.add(Dropout(0.2))\n",
    "model_3c.add(Dense(95, activation='relu'))\n",
    "model_3c.add(Dropout(0.2))\n",
    "model_3c.add(Dense(60, activation='relu'))\n",
    "model_3c.add(Dropout(0.2))\n",
    "model_3c.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "model_3c.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Row 4\n",
    "model_3d = Sequential()\n",
    "model_3d.add(Dense(250, input_dim=144, activation='relu'))\n",
    "model_3d.add(Dropout(0.2))\n",
    "model_3d.add(Dense(95, activation='relu'))\n",
    "model_3d.add(Dropout(0.2))\n",
    "model_3d.add(Dense(60, activation='relu'))\n",
    "model_3d.add(Dropout(0.2))\n",
    "model_3d.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "model_3d.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3a, X_test_3a, y_train_3a, y_test_3a = train_test_split(X_3a, Y_3a, train_size=0.60,\n",
    "                                                                    test_size=0.40)\n",
    "\n",
    "\n",
    "#weights\n",
    "class_weights_3a = class_weight.compute_class_weight('balanced', np.unique(y_train_3a), y_train_3a)\n",
    "\n",
    "\n",
    "\n",
    "X_train_3b, X_test_3b, y_train_3b, y_test_3b = train_test_split(X_3b, Y_3b, train_size=0.60,\n",
    "                                                                    test_size=0.40)\n",
    "\n",
    "\n",
    "#weights\n",
    "class_weights_3b = class_weight.compute_class_weight('balanced', np.unique(y_train_3b), y_train_3b)\n",
    "\n",
    "\n",
    "X_train_3c, X_test_3c, y_train_3c, y_test_3c = train_test_split(X_3c, Y_3c, train_size=0.60,\n",
    "                                                                    test_size=0.40)\n",
    "\n",
    "#weights\n",
    "class_weights_3c = class_weight.compute_class_weight('balanced', np.unique(y_train_3c), y_train_3c)\n",
    "\n",
    "\n",
    "X_train_3d, X_test_3d, y_train_3d, y_test_3d = train_test_split(X_3d, Y_3d, train_size=0.60,\n",
    "                                                                    test_size=0.40)\n",
    "\n",
    "# weights\n",
    "class_weights_3d = class_weight.compute_class_weight('balanced', np.unique(y_train_3d), y_train_3d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3a.fit(X_train_3a, y_train_3a ,validation_data=(X_test_3a,y_test_3a), epochs=50, batch_size=200, \n",
    "             class_weight = class_weights_3a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3b.fit(X_train_3b, y_train_3b ,validation_data=(X_test_3b,y_test_3b), epochs=50, batch_size=200,\n",
    "            class_weight = class_weights_3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3c.fit(X_train_3c, y_train_3c ,validation_data=(X_test_3c,y_test_3c), epochs=50, batch_size=200,\n",
    "            class_weight = class_weights_3c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3d.fit(X_train_3d, y_train_3d ,validation_data=(X_test_3d,y_test_3d), epochs=50, batch_size=200,\n",
    "            class_weight = class_weights_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_3a.evaluate(X_test_3a, y_test_3a, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model_3a.metrics_names[1], scores[1]*100))\n",
    "# save model and architecture to single file\n",
    "model_3a.save(\"model_3a.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_3b.evaluate(X_test_3b, y_test_3b, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model_3b.metrics_names[1], scores[1]*100))\n",
    "# save model and architecture to single file\n",
    "model_3b.save(\"model_3b.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_3c.evaluate(X_test_3c, y_test_3c, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model_3c.metrics_names[1], scores[1]*100))\n",
    "# save model and architecture to single file\n",
    "model_3c.save(\"model_3c.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_3d.evaluate(X_test_3d, y_test_3d, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model_3d.metrics_names[1], scores[1]*100))\n",
    "# save model and architecture to single file\n",
    "model_3d.save(\"model_3d.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Deep Nested Neural Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = tf.keras.models.load_model('model_1.h5')\n",
    "model_2a = tf.keras.models.load_model('model_2a.h5')\n",
    "model_2b = tf.keras.models.load_model('model_2b.h5')\n",
    "model_3a = tf.keras.models.load_model('model_3a.h5')\n",
    "model_3b = tf.keras.models.load_model('model_3b.h5')\n",
    "model_3c = tf.keras.models.load_model('model_3c.h5')\n",
    "model_3d = tf.keras.models.load_model('model_3d.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(\"test_final.csv\")\n",
    "test_dataset = test_dataset.iloc[:, 1:146]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_dataset.iloc[:, 0:144]\n",
    "test_y = test_dataset.iloc[:, -1]\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
